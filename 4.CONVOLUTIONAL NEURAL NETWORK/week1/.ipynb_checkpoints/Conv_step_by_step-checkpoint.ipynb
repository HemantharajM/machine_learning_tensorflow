{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONVOLUTIONAL NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to Course 4's first assignment! In this assignment, you will implement convolutional (CONV) and pooling (POOL) layers in numpy, including both forward propagation and (optionally) backward propagation. \n",
    "\n",
    "**Notation**:\n",
    "- Superscript $[l]$ denotes an object of the $l^{th}$ layer. \n",
    "    - Example: $a^{[4]}$ is the $4^{th}$ layer activation. $W^{[5]}$ and $b^{[5]}$ are the $5^{th}$ layer parameters.\n",
    "\n",
    "\n",
    "- Superscript $(i)$ denotes an object from the $i^{th}$ example. \n",
    "    - Example: $x^{(i)}$ is the $i^{th}$ training example input.\n",
    "    \n",
    "    \n",
    "- Lowerscript $i$ denotes the $i^{th}$ entry of a vector.\n",
    "    - Example: $a^{[l]}_i$ denotes the $i^{th}$ entry of the activations in layer $l$, assuming this is a fully connected (FC) layer.\n",
    "    \n",
    "    \n",
    "- $n_H$, $n_W$ and $n_C$ denote respectively the height, width and number of channels of a given layer. If you want to reference a specific layer $l$, you can also write $n_H^{[l]}$, $n_W^{[l]}$, $n_C^{[l]}$. \n",
    "- $n_{H_{prev}}$, $n_{W_{prev}}$ and $n_{C_{prev}}$ denote respectively the height, width and number of channels of the previous layer. If referencing a specific layer $l$, this could also be denoted $n_H^{[l-1]}$, $n_W^{[l-1]}$, $n_C^{[l-1]}$. \n",
    "\n",
    "We assume that you are already familiar with `numpy` and/or have completed the previous courses of the specialization. Let's get started!|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py \n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "###You will be implementing the building blocks of a convolutional neural network! Each function you will implement will have detailed instructions that will walk you through the steps needed:\n",
    "\n",
    "##Convolution functions, including:\n",
    " ###-Zero Padding\n",
    " ###- Convolve window \n",
    " ###- Convolution forward\n",
    " ###- Convolution backward (optional)\n",
    "##Pooling functions, including:\n",
    " ###- Pooling forward\n",
    " ###- Create mask \n",
    " ###- Distribute value\n",
    " ###- Pooling backward (optional)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero padding adds zeros around the border"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pad(X, pad):\n",
    "    \n",
    "    \"\"\"\n",
    "    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image.\n",
    "    \n",
    "    Argument:\n",
    "    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n",
    "    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
    "    \n",
    "    Returns:\n",
    "    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)\n",
    "    \"\"\"\n",
    "    X_pad = np.pad(X, ((0,0),(pad, pad), (pad, pad), (0,0)), 'constant', constant_values = 0)\n",
    "    \n",
    "    return X_pad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape = (4, 3, 3, 2)\n",
      "x_pad.shape = (4, 7, 7, 2)\n",
      "x[1,1] = [[ 0.90085595 -0.68372786]\n",
      " [-0.12289023 -0.93576943]\n",
      " [-0.26788808  0.53035547]]\n",
      "x_pad[1,1] = [[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff753b39940>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAACuCAYAAABUfpQYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSDOzFP8HYkJMRzjq+iUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff753d929e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(4, 3, 3, 2)\n",
    "x_pad = zero_pad(x,2)\n",
    "\n",
    "print (\"x.shape =\", x.shape)\n",
    "print (\"x_pad.shape =\", x_pad.shape)\n",
    "print (\"x[1,1] =\", x[1,1])\n",
    "print (\"x_pad[1,1] =\", x_pad[1,1])\n",
    "\n",
    "fig, axarr = plt.subplots(1, 2)\n",
    "axarr[0].set_title('x')\n",
    "axarr[0].imshow(x[0,:,:,0])\n",
    "axarr[1].set_title('x_pad')\n",
    "axarr[1].imshow(x_pad[0,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### single step convolution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    \n",
    "    \"\"\"\n",
    "    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation \n",
    "    of the previous layer.\n",
    "    \n",
    "    Arguments:\n",
    "    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\n",
    "    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\n",
    "    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
    "    \n",
    "    Returns:\n",
    "    Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data\n",
    "    \"\"\"\"\"\n",
    "    Z = np.multiply(a_slice_prev , W) \n",
    "    \n",
    "    Z = np.sum(Z) + float(b)\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = -6.99908945068\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "a_slice_prev = np.random.randn(4, 4, 3)\n",
    "W = np.random.randn(4, 4, 3)\n",
    "b = np.random.randn(1, 1, 1)\n",
    "\n",
    "Z = conv_single_step(a_slice_prev, W, b)\n",
    "\n",
    "print(\"Z =\", Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolution neural network - forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
    "    hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
    "        \n",
    "    Returns:\n",
    "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward() function\n",
    "    \"\"\"\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    pad = hparameters[\"pad\"]\n",
    "    stride  = hparameters[\"stride\"]\n",
    "    f = W.shape[0]\n",
    "    \n",
    "    vert_start = 0\n",
    "    vert_end = ((n_H_prev + 2* pad - f) // stride)+1\n",
    "    \n",
    "    horiz_start = 0\n",
    "    horiz_end = ((n_W_prev + 2*pad -f) // stride) + 1\n",
    "    \n",
    "    n_C = W.shape[3]\n",
    "    \n",
    "    Z = np.zeros([m,vert_end,horiz_end,n_C])        \n",
    " \n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev_pad.shape\n",
    "                  \n",
    "    for i in range(m): \n",
    "        for l in range(n_C):\n",
    "            for j in range(vert_end):       \n",
    "                for k in range(horiz_end):\n",
    "                    j_step = j * stride\n",
    "                    k_step = k * stride\n",
    "                    s = conv_single_step(A_prev_pad[i,j_step:j_step+f, k_step:k_step+f,:], W[:,:,:,l], b[:,:,:,l])\n",
    "                    \n",
    "                    Z[i,j, k,l] = s\n",
    "                    \n",
    "    \n",
    "    n_H = vert_end\n",
    "    n_W = horiz_end\n",
    "    # Making sure your output shape is correct\n",
    "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Save information in \"cache\" for the backprop\n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "    \n",
    "    return Z, cache\n",
    "                \n",
    "               \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z's mean = 0.0489952035289\n",
      "Z[3,2,1] = [-0.61490741 -6.7439236  -2.55153897  1.75698377  3.56208902  0.53036437\n",
      "  5.18531798  8.75898442]\n",
      "cache_conv[0][1][2][3] = [-0.20075807  0.18656139  0.41005165]\n",
      "(10, 4, 4, 8)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10,4,4,3)\n",
    "W = np.random.randn(2,2,3,8)\n",
    "b = np.random.randn(1,1,1,8)\n",
    "hparameters = {\"pad\" : 2,\n",
    "               \"stride\": 2}\n",
    "\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "print(\"Z's mean =\", np.mean(Z))\n",
    "print(\"Z[3,2,1] =\", Z[3,2,1])\n",
    "print(\"cache_conv[0][1][2][3] =\", cache_conv[0][1][2][3])\n",
    "print(Z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling layer reduce height and width of the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward pooling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_forward(A_prev, hparameters, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the forward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    hparameters -- python dictionary containing \"f\" and \"stride\"\n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters \n",
    "    \"\"\"\n",
    "    (m, n_H, n_W, n_C) = A_prev.shape\n",
    "    \n",
    "    f = hparameters['f']\n",
    "    stride = hparameters['stride']\n",
    "    \n",
    "    n_H_next = ((n_H - f) // stride )+ 1 \n",
    "    n_W_next = ((n_W - f) // stride ) + 1\n",
    "    \n",
    "    A = np.zeros([m, n_H_next, n_W_next, n_C])\n",
    "    \n",
    "    for i in range(m):\n",
    "        for j in range(n_C):\n",
    "            \n",
    "            for v in range(n_H_next):\n",
    "                for h in range(n_W_next):\n",
    "                    \n",
    "                    v_step = v * stride\n",
    "                    h_step = h * stride\n",
    "                    \n",
    "                    if mode == 'max' :\n",
    "                        A[i,v,h,j] = np.max(A_prev[i,v_step:v_step+f,h_step:h_step+f,j])\n",
    "                    else :\n",
    "                        A[i,v,h,j] = np.mean(A_prev[i,v_step:v_step+f, h_step:h_step +f , j])\n",
    "                    \n",
    "            \n",
    "    cache = (A_prev, hparameters)\n",
    "    \n",
    "    \n",
    "    return A, cache\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "A = [[[[ 1.74481176  0.86540763  1.13376944]]]\n",
      "\n",
      "\n",
      " [[[ 1.13162939  1.51981682  2.18557541]]]]\n",
      "\n",
      "mode = average\n",
      "A = [[[[ 0.02105773 -0.20328806 -0.40389855]]]\n",
      "\n",
      "\n",
      " [[[-0.22154621  0.51716526  0.48155844]]]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2, 4, 4, 3)\n",
    "hparameters = {\"stride\" : 2, \"f\": 3}\n",
    "\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "print(\"mode = max\")\n",
    "print(\"A =\", A)\n",
    "print()\n",
    "A, cache = pool_forward(A_prev, hparameters, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print(\"A =\", A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
